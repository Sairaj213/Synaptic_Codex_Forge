{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DnyeJZZOzgZT"
      },
      "outputs": [],
      "source": [
        "!pip -q install bitsandbytes streamlit pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WqWLimjFztwV"
      },
      "outputs": [],
      "source": [
        "!mkdir -p core utils ui"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3_O5qRp40BeA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u0000\u0000i\u0000n\u0000d\u0000o\u0000w\u0000s\u0000 \u0000S\u0000u\u0000b\u0000s\u0000y\u0000s\u0000t\u0000e\u0000m\u0000 \u0000f\u0000o\u0000r\u0000 \u0000L\u0000i\u0000n\u0000u\u0000x\u0000 \u0000h\u0000a\u0000s\u0000 \u0000n\u0000o\u0000 \u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000e\u0000d\u0000 \u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000s\u0000.\u0000\n",
            "\u0000Y\u0000o\u0000u\u0000 \u0000c\u0000a\u0000n\u0000 \u0000r\u0000e\u0000s\u0000o\u0000l\u0000v\u0000e\u0000 \u0000t\u0000h\u0000i\u0000s\u0000 \u0000b\u0000y\u0000 \u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000i\u0000n\u0000g\u0000 \u0000a\u0000 \u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000 \u0000w\u0000i\u0000t\u0000h\u0000 \u0000t\u0000h\u0000e\u0000 \u0000i\u0000n\u0000s\u0000t\u0000r\u0000u\u0000c\u0000t\u0000i\u0000o\u0000n\u0000s\u0000 \u0000b\u0000e\u0000l\u0000o\u0000w\u0000:\u0000\n",
            "\u0000\n",
            "\u0000U\u0000s\u0000e\u0000 \u0000'\u0000w\u0000s\u0000l\u0000.\u0000e\u0000x\u0000e\u0000 \u0000-\u0000-\u0000l\u0000i\u0000s\u0000t\u0000 \u0000-\u0000-\u0000o\u0000n\u0000l\u0000i\u0000n\u0000e\u0000'\u0000 \u0000t\u0000o\u0000 \u0000l\u0000i\u0000s\u0000t\u0000 \u0000a\u0000v\u0000a\u0000i\u0000l\u0000a\u0000b\u0000l\u0000e\u0000 \u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000s\u0000\n",
            "\u0000a\u0000n\u0000d\u0000 \u0000'\u0000w\u0000s\u0000l\u0000.\u0000e\u0000x\u0000e\u0000 \u0000-\u0000-\u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000 \u0000<\u0000D\u0000i\u0000s\u0000t\u0000r\u0000o\u0000>\u0000'\u0000 \u0000t\u0000o\u0000 \u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000.\u0000\n",
            "\u0000"
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "Command 'b'touch core/__init__.py\\ntouch utils/__init__.py\\ntouch ui/__init__.py\\n#touch ui/tabs/__init__.py\\n'' returned non-zero exit status 1.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbash\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtouch core/__init__.py\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mtouch utils/__init__.py\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mtouch ui/__init__.py\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m#touch ui/tabs/__init__.py\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:2565\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2564\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2568\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2569\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\magics\\script.py:160\u001b[39m, in \u001b[36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[39m\u001b[34m(line, cell)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    159\u001b[39m     line = script\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\magics\\script.py:343\u001b[39m, in \u001b[36mScriptMagics.shebang\u001b[39m\u001b[34m(self, line, cell)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.raise_error \u001b[38;5;129;01mand\u001b[39;00m p.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[32m    340\u001b[39m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[32m    341\u001b[39m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[32m    342\u001b[39m     rc = p.returncode \u001b[38;5;129;01mor\u001b[39;00m -\u001b[32m9\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
            "\u001b[31mCalledProcessError\u001b[39m: Command 'b'touch core/__init__.py\\ntouch utils/__init__.py\\ntouch ui/__init__.py\\n#touch ui/tabs/__init__.py\\n'' returned non-zero exit status 1."
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "touch core/__init__.py\n",
        "touch utils/__init__.py\n",
        "touch ui/__init__.py\n",
        "#touch ui/tabs/__init__.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing core/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile core/__init__.py\n",
        "# Core module initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ui/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ui/__init__.py\n",
        "# UI module initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing utils/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils/__init__.py\n",
        "# Utils module initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d17hML8LK6gs",
        "outputId": "920657cd-1e5d-4298-fed8-d3e373f23a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.py\n",
        "import os\n",
        "\n",
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "PROJECT_OUTPUT_DIR = \"generated_projects\"\n",
        "DEFAULT_PROJECT_STRUCTURE = {\n",
        "    \"folders\": [\n",
        "        \"assets\",\n",
        "        \"core\",\n",
        "        \"utils\",\n",
        "        \"ui\"\n",
        "    ],\n",
        "    \"files\": {\n",
        "        \"main.py\": \"Placeholder for main application entry point.\",\n",
        "        \"config.py\": \"Placeholder for project configuration.\",\n",
        "        \"README.md\": \"Placeholder for project documentation.\",\n",
        "        \"requirements.txt\": \"Placeholder for project dependencies.\",\n",
        "        \"core/__init__.py\": \"# Makes 'core' a Python package\",\n",
        "        \"core/logic.py\": \"Placeholder for core business logic.\",\n",
        "        \"utils/__init__.py\": \"# Makes 'utils' a Python package\",\n",
        "        \"utils/helpers.py\": \"Placeholder for utility functions.\",\n",
        "        \"ui/__init__.py\": \"# Makes 'ui' a Python package\",\n",
        "        \"ui/display.py\": \"Placeholder for UI or display logic.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "DEFAULT_LLM_PARAMS = {\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_new_tokens\": 4096,\n",
        "    \"debug_mode\": True\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmUojeviK7S2",
        "outputId": "d8d0a92e-82d1-4bea-e8c3-b79579345361"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing utils/session_state.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils/session_state.py\n",
        "import streamlit as st\n",
        "\n",
        "def initialize_state():\n",
        "\n",
        "    if 'project_plan' not in st.session_state:\n",
        "        st.session_state.project_plan = None\n",
        "\n",
        "    if 'generation_log' not in st.session_state:\n",
        "        st.session_state.generation_log = []\n",
        "\n",
        "    if 'project_zip_path' not in st.session_state:\n",
        "        st.session_state.project_zip_path = None\n",
        "\n",
        "    if 'generated_code' not in st.session_state:\n",
        "        st.session_state.generated_code = {}\n",
        "\n",
        "    if 'project_stats' not in st.session_state:\n",
        "        st.session_state.project_stats = {\n",
        "            \"total_files\": 0,\n",
        "            \"total_lines\": 0,\n",
        "            \"time_taken\": 0.0,\n",
        "            \"files_list\": []\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ77zQTNK-ni",
        "outputId": "1ae69a00-3874-44fa-b568-1e7daf6c5462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing utils/file_manager.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils/file_manager.py\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import streamlit as st\n",
        "from config import PROJECT_OUTPUT_DIR\n",
        "\n",
        "def create_project_structure(project_name, structure_plan):\n",
        " \n",
        "    project_path = os.path.join(PROJECT_OUTPUT_DIR, project_name)\n",
        "\n",
        "    if os.path.exists(project_path):\n",
        "        shutil.rmtree(project_path)\n",
        "\n",
        "    os.makedirs(project_path, exist_ok=True)\n",
        "\n",
        "    log = []\n",
        "\n",
        "    if \"folders\" in structure_plan:\n",
        "        for folder in structure_plan[\"folders\"]:\n",
        "            folder_path = os.path.join(project_path, folder)\n",
        "            os.makedirs(folder_path, exist_ok=True)\n",
        "            log.append(f\"Created folder: {folder_path}\")\n",
        "\n",
        "    if \"files\" in structure_plan:\n",
        "        for file_path, description in structure_plan[\"files\"].items():\n",
        "            full_file_path = os.path.join(project_path, file_path)\n",
        "            os.makedirs(os.path.dirname(full_file_path), exist_ok=True)\n",
        "            with open(full_file_path, 'w') as f:\n",
        "                if description.startswith(\"Placeholder\"):\n",
        "                    f.write(f\"# {description}\\n\")\n",
        "                else:\n",
        "                     f.write(\"\") \n",
        "            log.append(f\"Created empty file: {full_file_path}\")\n",
        "\n",
        "    return project_path, log\n",
        "\n",
        "def save_code_to_file(file_path, code):\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(code)\n",
        "        return True, f\"Successfully saved code to {file_path}\"\n",
        "    except Exception as e:\n",
        "        return False, f\"Error saving file {file_path}: {e}\"\n",
        "\n",
        "def zip_project_folder(project_path, project_name):\n",
        " \n",
        "    zip_file_name = f\"{project_name}.zip\"\n",
        "    zip_file_path = os.path.join(PROJECT_OUTPUT_DIR, zip_file_name)\n",
        "\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for root, dirs, files in os.walk(project_path):\n",
        "                for file in files:\n",
        "                    full_path = os.path.join(root, file)\n",
        "                    relative_path = os.path.relpath(full_path, project_path)\n",
        "                    zipf.write(full_path, relative_path)\n",
        "\n",
        "        return zip_file_path, f\"Project zipped successfully: {zip_file_path}\"\n",
        "    except Exception as e:\n",
        "        return None, f\"Error zipping project: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7eba3sqLFmd",
        "outputId": "beb40b4c-e25c-40a6-b4ca-63193c697003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing core/prompts.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile core/prompts.py\n",
        "from config import DEFAULT_PROJECT_STRUCTURE\n",
        "\n",
        "def get_planner_system_prompt():\n",
        "\n",
        "    file_list = \"\\n\".join([f\"- {f}\" for f in DEFAULT_PROJECT_STRUCTURE['files'].keys()])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an expert software architect. Your job is to analyze a user's request and create a concise, one-sentence functional description for *each file* in a predefined project structure.\n",
        "\n",
        "**Your Task:**\n",
        "Based on the user's request, you must provide a new description for *every file* listed below.\n",
        "Do not add, remove, or rename any files.\n",
        "If a file is not needed for the user's request, you *must* describe it as 'Placeholder' or 'Basic boilerplate'.\n",
        "\n",
        "**Project File Structure:**\n",
        "{file_list}\n",
        "\n",
        "**Response Format:**\n",
        "You *must* respond *only* with a Python dictionary. The keys must be the filenames (e.g., 'main.py') and the values must be the new one-sentence descriptions.\n",
        "\n",
        "**Example Response:**\n",
        "{{\n",
        "    \"main.py\": \"Main entry point to run the application.\",\n",
        "    \"config.py\": \"Stores configuration variables like API keys.\",\n",
        "    \"README.md\": \"Basic boilerplate documentation.\",\n",
        "    \"requirements.txt\": \"Placeholder. No external libraries needed.\",\n",
        "    \"core/__init__.py\": \"# Makes 'core' a Python package\",\n",
        "    \"core/logic.py\": \"Contains the core function to process the data.\",\n",
        "    \"utils/__init__.py\": \"# Makes 'utils' a Python package\",\n",
        "    \"utils/helpers.py\": \"Placeholder. No helper functions needed.\",\n",
        "    \"ui/__init__.py\": \"# Makes 'ui' a Python package\",\n",
        "    \"ui/display.py\": \"Contains the function to print the output.\"\n",
        "}}\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "\n",
        "def get_coder_system_prompt():\n",
        "\n",
        "    prompt = \"\"\"\n",
        "You are an expert Python developer. Your *only* job is to write complete, correct, and production-ready code for a *single file*.\n",
        "\n",
        "**Rules:**\n",
        "1.  You *must* respond *only* with the raw code for the file requested.\n",
        "2.  Do *not* write *any* explanations, apologies, or text before or after the code.\n",
        "3.  Do *not* use Markdown (e.g., ```python ... ```).\n",
        "4.  Ensure the code is complete and functional.\n",
        "5.  If the task is 'Placeholder', just write a simple comment (e.g., '# This file is a placeholder.')\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "\n",
        "def get_coder_user_prompt(project_plan, file_name):\n",
        "\n",
        "    if file_name not in project_plan[\"files\"]:\n",
        "        return f\"Error: File '{file_name}' not found in project plan.\"\n",
        "\n",
        "    task_description = project_plan[\"files\"][file_name]\n",
        "    plan_context = \"\\n\".join([f\"- {f}: {desc}\" for f, desc in project_plan[\"files\"].items()])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "**Overall Project Plan:**\n",
        "{plan_context}\n",
        "\n",
        "---\n",
        "\n",
        "**Your Specific Task:**\n",
        "Write the *complete code* for the file: `{file_name}`\n",
        "**File Description:** \"{task_description}\"\n",
        "\"\"\"\n",
        "    return prompt.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMtZcI_mLIsq",
        "outputId": "f4f3df92-10c6-47df-c364-d0bf58aa27f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing core/model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile core/model.py\n",
        "import torch\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from config import MODEL_ID\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_ID,\n",
        "            quantization_config=bnb_config,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "\n",
        "        model.config.use_cache = False\n",
        "\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading model: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def generate_text(system_prompt, user_prompt, model, tokenizer, llm_params):\n",
        "\n",
        "    if model is None or tokenizer is None:\n",
        "        return \"Error: Model not loaded.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "    model_inputs = encodeds.to(model.device)\n",
        "    input_token_length = model_inputs.shape[1]\n",
        "\n",
        "    try:\n",
        "        generated_ids = model.generate(\n",
        "            model_inputs,\n",
        "            max_new_tokens=llm_params.get(\"max_new_tokens\", 4096),\n",
        "            temperature=llm_params.get(\"temperature\", 0.7),\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        new_token_ids = generated_ids[0, input_token_length:]\n",
        "\n",
        "        reply = tokenizer.decode(new_token_ids, skip_special_tokens=True)\n",
        "\n",
        "        return reply.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error during text generation: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBZPSjhOLNS-",
        "outputId": "5f905f38-9874-4922-e3c4-64dbbd021826"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing core/agent.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile core/agent.py\n",
        "import os\n",
        "import time\n",
        "import ast\n",
        "import re\n",
        "import streamlit as st\n",
        "from core.model import generate_text\n",
        "from core.prompts import get_planner_system_prompt, get_coder_system_prompt, get_coder_user_prompt\n",
        "from utils.file_manager import create_project_structure, save_code_to_file, zip_project_folder\n",
        "from config import DEFAULT_PROJECT_STRUCTURE\n",
        "\n",
        "def log_message(message, type=\"info\", placeholder=None):\n",
        "\n",
        "    st.session_state.generation_log.append(message)\n",
        "\n",
        "    if placeholder:\n",
        "        log_func = getattr(placeholder, type, placeholder.info)\n",
        "        log_func(message)\n",
        "\n",
        "def run_planner(user_request, model, tokenizer, llm_params, log_placeholder):\n",
        "\n",
        "    log_message(\"Calling 'Planner' agent...\", placeholder=log_placeholder)\n",
        "    system_prompt = get_planner_system_prompt()\n",
        "\n",
        "    response = generate_text(system_prompt, user_request, model, tokenizer, llm_params)\n",
        "\n",
        "    if \"Error\" in response:\n",
        "        log_message(response, \"error\", placeholder=log_placeholder)\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        start_index = response.find('{')\n",
        "        end_index = response.rfind('}')\n",
        "\n",
        "        if start_index == -1 or end_index == -1:\n",
        "            raise ValueError(\"Could not find a dictionary in the model's response.\")\n",
        "\n",
        "        plan_str = response[start_index : end_index + 1]\n",
        "        plan_str = plan_str.replace(\"Placeholder\", \"'Placeholder'\").replace(\"Basic boilerplate\", \"'Basic boilerplate'\")\n",
        "        project_plan = ast.literal_eval(plan_str)\n",
        "\n",
        "        if not isinstance(project_plan, dict):\n",
        "            raise ValueError(\"Planner did not return a valid dictionary structure.\")\n",
        "\n",
        "        full_plan = {\n",
        "            \"folders\": DEFAULT_PROJECT_STRUCTURE[\"folders\"],\n",
        "            \"files\": project_plan\n",
        "        }\n",
        "\n",
        "        st.session_state.project_plan = full_plan\n",
        "        log_message(\"âœ… Project plan generated successfully.\", \"success\", placeholder=log_placeholder)\n",
        "        return full_plan\n",
        "\n",
        "    except Exception as e:\n",
        "        log_message(f\"Error parsing planner response: {e}\\nResponse was: {response}\", \"error\", placeholder=log_placeholder)\n",
        "        return None\n",
        "\n",
        "def clean_code_output(raw_code):\n",
        "    start_fence = raw_code.find(\"```\")\n",
        "    if start_fence == -1:\n",
        "        return raw_code.strip()\n",
        "    first_newline = raw_code.find('\\n', start_fence)\n",
        "    if first_newline == -1:\n",
        "        code_start = start_fence + 3\n",
        "    else:\n",
        "        code_start = first_newline + 1\n",
        "    last_fence = raw_code.rfind(\"```\")\n",
        "    if last_fence == -1 or last_fence <= code_start:\n",
        "        return raw_code[code_start:].strip()\n",
        "    return raw_code[code_start : last_fence].strip()\n",
        "\n",
        "\n",
        "def run_coder_and_saver(project_path, project_plan, model, tokenizer, llm_params, log_placeholder):\n",
        " \n",
        "    log_message(\"Calling 'Coder' agent for each file...\", placeholder=log_placeholder)\n",
        "\n",
        "    total_lines = 0\n",
        "    generated_files_list = []\n",
        "    files_to_generate = project_plan.get(\"files\", {})\n",
        "    if not files_to_generate:\n",
        "        log_message(\"No files found in the project plan.\", \"warning\", placeholder=log_placeholder)\n",
        "        return\n",
        "\n",
        "    st.session_state.generated_code = {}\n",
        "\n",
        "    for file_name, task_description in files_to_generate.items():\n",
        "        log_message(f\"Generating code for: {file_name}...\", placeholder=log_placeholder)\n",
        "\n",
        "        system_prompt = get_coder_system_prompt()\n",
        "        user_prompt = get_coder_user_prompt(project_plan, file_name)\n",
        "        raw_output = generate_text(system_prompt, user_prompt, model, tokenizer, llm_params)\n",
        "        generated_code = clean_code_output(raw_output)\n",
        "\n",
        "        if \"Error\" in raw_output:\n",
        "            log_message(f\"Error generating code for {file_name}: {raw_output}\", \"error\", placeholder=log_placeholder)\n",
        "            continue\n",
        "\n",
        "        if not generated_code and \"Placeholder\" not in task_description:\n",
        "            log_message(f\"Error generating code for {file_name}: Model returned empty string.\", \"error\", placeholder=log_placeholder)\n",
        "            continue\n",
        "\n",
        "        full_file_path = os.path.join(project_path, file_name)\n",
        "        success, message = save_code_to_file(full_file_path, generated_code)\n",
        "\n",
        "        if success:\n",
        "            lines = len(generated_code.split('\\n'))\n",
        "            total_lines += lines\n",
        "            generated_files_list.append({\"File\": file_name, \"Status\": \"âœ… Generated\", \"Lines\": lines})\n",
        "            log_message(f\"âœ… Saved {file_name} ({lines} lines).\", \"success\", placeholder=log_placeholder)\n",
        "            st.session_state.generated_code[file_name] = generated_code\n",
        "        else:\n",
        "            log_message(message, \"error\", placeholder=log_placeholder)\n",
        "            generated_files_list.append({\"File\": file_name, \"Status\": \"âŒ Error\", \"Lines\": 0})\n",
        "\n",
        "    return total_lines, generated_files_list\n",
        "\n",
        "\n",
        "def run_generation_pipeline(user_request, project_name, model, tokenizer, llm_params, log_placeholder):\n",
        "\n",
        "    start_time = time.time()\n",
        "    st.session_state.generation_log = []\n",
        "    st.session_state.project_plan = None\n",
        "    st.session_state.project_zip_path = None\n",
        "    st.session_state.generated_code = {}\n",
        "    st.session_state.project_stats = {\"total_files\": 0, \"total_lines\": 0, \"time_taken\": 0.0, \"files_list\": []}\n",
        "    project_plan = run_planner(user_request, model, tokenizer, llm_params, log_placeholder)\n",
        "    if project_plan is None:\n",
        "        return\n",
        "\n",
        "    log_message(\"Creating project directory structure...\", placeholder=log_placeholder)\n",
        "    project_path, logs = create_project_structure(project_name, project_plan)\n",
        "    for log in logs:\n",
        "        st.session_state.generation_log.append(log) \n",
        "        log_message(log, \"info\", placeholder=log_placeholder) \n",
        "    log_message(f\"âœ… Project structure created at: {project_path}\", \"success\", placeholder=log_placeholder)\n",
        "    total_lines, files_list = run_coder_and_saver(project_path, project_plan, model, tokenizer, llm_params, log_placeholder)\n",
        "    log_message(\"Zipping project folder for download...\", placeholder=log_placeholder)\n",
        "    zip_path, message = zip_project_folder(project_path, project_name)\n",
        "    if zip_path:\n",
        "        st.session_state.project_zip_path = zip_path\n",
        "        log_message(f\"âœ… Project zipped successfully!\", \"success\", placeholder=log_placeholder)\n",
        "    else:\n",
        "        log_message(message, \"error\", placeholder=log_placeholder)\n",
        "\n",
        "    end_time = time.time()\n",
        "    time_taken = end_time - start_time\n",
        "    st.session_state.project_stats = {\n",
        "        \"total_files\": len(files_list),\n",
        "        \"total_lines\": total_lines,\n",
        "        \"time_taken\": round(time_taken, 2),\n",
        "        \"files_list\": files_list\n",
        "    }\n",
        "    log_message(f\"--- Pipeline Finished in {time_taken:.2f} seconds ---\", \"success\", placeholder=log_placeholder)\n",
        "    log_placeholder.balloons()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNAz-emBLQ-6",
        "outputId": "e157b497-5211-45d6-948e-dbb14012c3d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ui/sidebar.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ui/sidebar.py\n",
        "import streamlit as st\n",
        "from config import DEFAULT_LLM_PARAMS, MODEL_ID\n",
        "\n",
        "def draw_sidebar():\n",
        "\n",
        "    st.sidebar.title(\"âš™ï¸ Control Panel\")\n",
        "    st.sidebar.markdown(\"Configure the agent and LLM parameters.\")\n",
        "    st.sidebar.header(\"Project Settings\")\n",
        "    project_name = st.sidebar.text_input(\n",
        "        \"Project Name\",\n",
        "        value=\"my_awesome_project\",\n",
        "        help=\"The name of the folder for your generated project.\"\n",
        "    )\n",
        "\n",
        "    st.sidebar.header(\"LLM Parameters\")\n",
        "    st.sidebar.text_input(\"Model\", value=MODEL_ID, disabled=True)\n",
        "\n",
        "    temperature = st.sidebar.slider(\n",
        "        \"Temperature\",\n",
        "        min_value=0.0,\n",
        "        max_value=1.0,\n",
        "        value=DEFAULT_LLM_PARAMS[\"temperature\"],\n",
        "        step=0.05,\n",
        "        help=\"Controls randomness. Lower is more deterministic, higher is more creative.\"\n",
        "    )\n",
        "\n",
        "    max_new_tokens = st.sidebar.slider(\n",
        "        \"Max New Tokens\",\n",
        "        min_value=512,\n",
        "        max_value=8192,\n",
        "        value=DEFAULT_LLM_PARAMS[\"max_new_tokens\"],\n",
        "        step=256,\n",
        "        help=\"Maximum number of tokens the model can generate for each file.\"\n",
        "    )\n",
        "\n",
        "    st.sidebar.header(\"Agent Controls\")\n",
        "    debug_mode = st.sidebar.checkbox(\n",
        "        \"Debug Mode\",\n",
        "        value=DEFAULT_LLM_PARAMS[\"debug_mode\"],\n",
        "        help=\"Show extra debug information in the logs.\"\n",
        "    )\n",
        "\n",
        "    st.sidebar.header(\"Utility\")\n",
        "    if st.sidebar.button(\"Clear Session Cache\"):\n",
        "        st.session_state.clear()\n",
        "        st.rerun()\n",
        "\n",
        "    llm_params = {\n",
        "        \"temperature\": temperature,\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"debug_mode\": debug_mode\n",
        "    }\n",
        "\n",
        "    return project_name, llm_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAuD6kYCLVvw",
        "outputId": "2101c328-dea8-42bb-d035-915c5fea5082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ui/widgets.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ui/widgets.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def draw_status_monitor():\n",
        "\n",
        "    st.subheader(\"Live Agent Monitor\")\n",
        "    log_container = st.container(height=300, border=True)\n",
        "\n",
        "    if 'generation_log' not in st.session_state or not st.session_state.generation_log:\n",
        "      \n",
        "        log_container.info(\"Agent logs will appear here...\")\n",
        "      \n",
        "        return\n",
        "\n",
        "    for log_entry in reversed(st.session_state.generation_log):\n",
        "        if log_entry.startswith(\"âœ…\"):\n",
        "            log_container.success(log_entry)\n",
        "        elif log_entry.startswith(\"Error\") or log_entry.startswith(\"âŒ\"):\n",
        "            log_container.error(log_entry)\n",
        "        elif log_entry.startswith(\"---\"):\n",
        "            log_container.markdown(f\"**{log_entry}**\")\n",
        "        else:\n",
        "            log_container.info(log_entry)\n",
        "\n",
        "def draw_code_viewer():\n",
        "\n",
        "    st.subheader(\"Generated Code Viewer\")\n",
        "\n",
        "    generated_code = st.session_state.get('generated_code', {})\n",
        "\n",
        "    if not generated_code:\n",
        "        st.info(\"No code has been generated yet. Run the agent from the 'Run' tab.\")\n",
        "        return\n",
        "\n",
        "    file_list = list(generated_code.keys())\n",
        "    selected_file = st.selectbox(\"Select a file to view:\", file_list)\n",
        "\n",
        "    if selected_file:\n",
        "        st.code(generated_code[selected_file], language=\"python\")\n",
        "\n",
        "def draw_project_stats_and_download():\n",
        "\n",
        "    st.subheader(\"Project Stats\")\n",
        "    stats = st.session_state.get('project_stats', {})\n",
        "\n",
        "    if not stats or stats[\"total_files\"] == 0:\n",
        "        st.info(\"No stats to display. Run the agent to generate a project.\")\n",
        "        return\n",
        "\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    col1.metric(\"Total Files\", stats.get(\"total_files\", 0))\n",
        "    col2.metric(\"Total Lines of Code\", stats.get(\"total_lines\", 0))\n",
        "    col3.metric(\"Time Taken (sec)\", stats.get(\"time_taken\", 0.0))\n",
        "\n",
        "    st.subheader(\"Generated Files\")\n",
        "    files_df = pd.DataFrame(stats.get(\"files_list\", []))\n",
        "    st.dataframe(files_df, use_container_width=True)\n",
        "    st.subheader(\"Download Project\")\n",
        "    zip_path = st.session_state.get('project_zip_path')\n",
        "\n",
        "    if zip_path and os.path.exists(zip_path):\n",
        "        with open(zip_path, \"rb\") as f:\n",
        "            st.download_button(\n",
        "                label=\"Download Project as .zip\",\n",
        "                data=f,\n",
        "                file_name=os.path.basename(zip_path),\n",
        "                mime=\"application/zip\",\n",
        "            )\n",
        "    else:\n",
        "        st.warning(\"No project ZIP file found. Please run the generation pipeline first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW0ivXzILXr5",
        "outputId": "3effc5c8-5354-4ca7-fbd4-c835bfdeffff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ui/tabs_layout.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ui/tabs_layout.py\n",
        "import streamlit as st\n",
        "from ui.widgets import draw_status_monitor, draw_code_viewer, draw_project_stats_and_download\n",
        "\n",
        "def draw_tabs():\n",
        "\n",
        "    tab1, tab2, tab3 = st.tabs([\n",
        "        \"ðŸ“ Plan & Monitor\",\n",
        "        \"VIEW: Code & Files\",\n",
        "        \"ðŸ“ˆ Stats & Download\"\n",
        "    ])\n",
        "\n",
        "    with tab1:\n",
        "        st.header(\"Project Plan & Generation Status\")\n",
        "\n",
        "        col1, col2 = st.columns([1, 1])\n",
        "\n",
        "        with col1:\n",
        "            st.subheader(\"Project Plan\")\n",
        "            plan_container = st.container(height=300, border=True)\n",
        "            plan = st.session_state.get('project_plan')\n",
        "\n",
        "            if plan:\n",
        "                plan_container.json(plan)\n",
        "            else:\n",
        "                plan_container.info(\"The project plan will appear here after generation.\")\n",
        "\n",
        "        with col2:\n",
        "            draw_status_monitor()\n",
        "\n",
        "    with tab2:\n",
        "        st.header(\"Review Generated Code\")\n",
        "        draw_code_viewer()\n",
        "\n",
        "    with tab3:\n",
        "        st.header(\"Project Statistics & Download\")\n",
        "        draw_project_stats_and_download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qcg7JHDLgxn",
        "outputId": "340c3858-4793-48bb-96c3-8ccd93a9ebfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "import streamlit as st\n",
        "from utils.session_state import initialize_state\n",
        "from ui.sidebar import draw_sidebar\n",
        "from ui.tabs_layout import draw_tabs\n",
        "from core.model import load_model\n",
        "from core.agent import run_generation_pipeline\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Synaptic Codex Forge\",\n",
        "    page_icon=\"ðŸ¤–\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "initialize_state()\n",
        "project_name, llm_params = draw_sidebar()\n",
        "\n",
        "st.title(\"ðŸ¤– Synaptic Codex Forge\")\n",
        "st.markdown(\"Powered by `mistralai/Mistral-7B-Instruct-v0.3`\")\n",
        "\n",
        "model_loaded = False\n",
        "with st.spinner(\"Loading Mistral-7B model... This may take a moment.\"):\n",
        "    model, tokenizer = load_model()\n",
        "\n",
        "if model is None or tokenizer is None:\n",
        "    st.error(\"Model failed to load. Please check your setup and network connection.\")\n",
        "else:\n",
        "    st.success(\"Model loaded successfully!\")\n",
        "    model_loaded = True\n",
        "\n",
        "st.header(\"What do you want to build?\")\n",
        "user_request = st.text_area(\n",
        "    \"Describe your project in detail:\",\n",
        "    height=150,\n",
        "    placeholder=\"e.g., 'A Python app that uses Flask to serve a single 'Hello World' API endpoint...'\"\n",
        ")\n",
        "\n",
        "if model_loaded:\n",
        "    if st.button(\"ðŸš€ Generate Project\", type=\"primary\", use_container_width=True):\n",
        "        if not user_request:\n",
        "            st.warning(\"Please describe your project first.\")\n",
        "        elif not project_name:\n",
        "            st.warning(\"Please enter a project name in the sidebar.\")\n",
        "        else:\n",
        "            st.subheader(\"Live Agent Monitor\")\n",
        "            log_placeholder = st.container(height=400, border=True)\n",
        "            run_generation_pipeline(\n",
        "                user_request,\n",
        "                project_name,\n",
        "                model,\n",
        "                tokenizer,\n",
        "                llm_params,\n",
        "                log_placeholder  \n",
        "            )\n",
        "            st.rerun()\n",
        "    st.divider()\n",
        "draw_tabs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bENyUKyMEJdk",
        "outputId": "4d9a6b3a-e074-4ef9-c311-34c7ff549549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://unbridged-flakily-lavera.ngrok-free.dev\" -> \"http://localhost:8501\">"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"34K1X9NzK5wXpXmklcPPrpoeqr3_65bWBoArzPwhAxVEhc96m\")\n",
        "!nohup streamlit run main.py --server.port 8501 &\n",
        "public_url = ngrok.connect(8501)\n",
        "public_url"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
